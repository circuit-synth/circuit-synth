# Circuit-Synth Autonomous Coordinator Configuration
# Multi-Provider Example

[coordinator]
max_concurrent_workers = 1
poll_interval_seconds = 30
worker_timeout_hours = 2

[github]
repo_owner = "circuit-synth"
repo_name = "circuit-synth"
issue_label = "rpi-auto"

# ============================================================================
# PROVIDER CONFIGURATIONS - Define multiple providers
# ============================================================================

# Provider: Claude CLI (local)
[providers.claude-cli]
enabled = true
command_template = [
    "claude",
    "-p", "{prompt_file}",
    "--model", "{model}",
    "--output-format", "stream-json",
    "--verbose",
    "--dangerously-skip-permissions"
]

# Provider: OpenRouter (API with many models)
[providers.openrouter]
enabled = false
api_key_env = "OPENROUTER_API_KEY"
command_template = [
    "curl", "-X", "POST",
    "https://openrouter.ai/api/v1/chat/completions",
    "-H", "Authorization: Bearer ${OPENROUTER_API_KEY}",
    "-H", "Content-Type: application/json",
    "-d", "@{prompt_file}",
    "--output", "{output_file}"
]

# Provider: OpenAI API
[providers.openai]
enabled = false
api_key_env = "OPENAI_API_KEY"
command_template = [
    "openai", "api", "chat.completions.create",
    "-m", "{model}",
    "--messages", "file:{prompt_file}",
    "--stream"
]

# Provider: Grok (xAI)
[providers.grok]
enabled = false
api_key_env = "XAI_API_KEY"
command_template = [
    "curl", "-X", "POST",
    "https://api.x.ai/v1/chat/completions",
    "-H", "Authorization: Bearer ${XAI_API_KEY}",
    "-H", "Content-Type: application/json",
    "-d", "@{prompt_file}"
]

# ============================================================================
# MODEL CATALOG - Define models with costs and capabilities
# ============================================================================

# Claude models
[[models]]
name = "claude-sonnet-4-5"
provider = "claude-cli"
cost_per_million_input = 3.0
cost_per_million_output = 15.0
context_window = 200000
capabilities = ["code", "reasoning", "vision"]

[[models]]
name = "claude-opus-4"
provider = "claude-cli"
cost_per_million_input = 15.0
cost_per_million_output = 75.0
context_window = 200000
capabilities = ["code", "reasoning", "vision", "complex"]

[[models]]
name = "claude-haiku-4"
provider = "claude-cli"
cost_per_million_input = 0.25
cost_per_million_output = 1.25
context_window = 200000
capabilities = ["code", "fast"]

# OpenRouter models (examples)
[[models]]
name = "anthropic/claude-3.5-sonnet"
provider = "openrouter"
cost_per_million_input = 3.0
cost_per_million_output = 15.0
context_window = 200000
capabilities = ["code", "reasoning"]

[[models]]
name = "deepseek/deepseek-chat"
provider = "openrouter"
cost_per_million_input = 0.14
cost_per_million_output = 0.28
context_window = 64000
capabilities = ["code", "cheap"]

[[models]]
name = "meta-llama/llama-3.1-405b-instruct"
provider = "openrouter"
cost_per_million_input = 2.7
cost_per_million_output = 2.7
context_window = 128000
capabilities = ["reasoning", "large"]

# OpenAI models
[[models]]
name = "gpt-4-turbo"
provider = "openai"
cost_per_million_input = 10.0
cost_per_million_output = 30.0
context_window = 128000
capabilities = ["code", "reasoning"]

[[models]]
name = "gpt-4o-mini"
provider = "openai"
cost_per_million_input = 0.15
cost_per_million_output = 0.6
context_window = 128000
capabilities = ["code", "fast", "cheap"]

# Grok models
[[models]]
name = "grok-beta"
provider = "grok"
cost_per_million_input = 5.0
cost_per_million_output = 15.0
context_window = 131072
capabilities = ["reasoning", "realtime"]

# ============================================================================
# MODEL ROUTING - Assign models to different workflow layers
# ============================================================================

[routing]
# Default model if no rules match
default_model = "claude-sonnet-4-5"

# Route by priority
[[routing.rules]]
condition = "priority == 0"  # p0 = critical
model = "claude-opus-4"
reason = "Use most capable model for critical tasks"

[[routing.rules]]
condition = "priority >= 3"  # p3+ = low priority
model = "claude-haiku-4"
reason = "Use fast/cheap model for low priority"

# Route by task type (from GitHub labels)
[[routing.rules]]
condition = "labels contains 'bug'"
model = "claude-sonnet-4-5"
reason = "Use reasoning model for bug fixes"

[[routing.rules]]
condition = "labels contains 'documentation'"
model = "claude-haiku-4"
reason = "Use fast model for docs"

[[routing.rules]]
condition = "labels contains 'complex'"
model = "claude-opus-4"
reason = "Use most capable for complex work"

# Route by estimated complexity (future: could use classifier)
[[routing.rules]]
condition = "estimated_tokens > 50000"
model = "claude-opus-4"
reason = "Large tasks need high context"

# Cost-saving rules
[[routing.rules]]
condition = "retry_count >= 2"
model = "claude-haiku-4"
reason = "If retrying, try cheaper model"

# Time-of-day routing (cost optimization)
[[routing.rules]]
condition = "hour >= 22 or hour <= 6"  # Night hours
model = "deepseek/deepseek-chat"  # Use cheaper OpenRouter model
reason = "Night jobs use budget models"

[worktree]
base_dir = "trees"
